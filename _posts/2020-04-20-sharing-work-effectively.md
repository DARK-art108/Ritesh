---
keywords: fastai
description: A few tips on sharing your work online effectively. 
title: Sharing your work online effectively
toc: false
comments: true
hide: false
badges: false
search_exclude: false
author: Sayak Paul
categories: [blogs, sharing]
nb_path: _notebooks/2020-04-20-sharing-work-effectively.ipynb
layout: notebook
---

<!--
#################################################
### THIS FILE WAS AUTOGENERATED! DO NOT EDIT! ###
#################################################
# file to edit: _notebooks/2020-04-20-sharing-work-effectively.ipynb
-->

<div class="container" id="notebook-container">
        
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Well, you have put a lot of blood and sweat into writing your latest blog post on Machine Learning. Don't let your struggle go in vain and let the world know about it. Sharing your blog posts across different channels not only gives you exposure but also may get you tremendous feedback on your work. In my personal experience, the feedback has been super useful for me to improve myself not only as a writer but also as a communicator. There can be times you might have missed out on a super important detail, or you might have unknowingly introduced a snazzy bug in the code listings of your blog -- those things could have been caught in the process of feedback interchange.</p>
<p>In this short article, I am going to enlist a few different ways to share your work and get feedback. Note your work can be anything starting from a crucial GitHub PR, to a weekend project. Although the following platforms and communities are mostly limited to Machine Learning, I hope this guide will be useful for tech bloggers in general.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Sharing-on-platforms/communities">Sharing on platforms/communities<a class="anchor-link" href="#Sharing-on-platforms/communities"> </a></h2><p>Before I start the sharing process, I generally create a Google Doc to effectively keep track of where I am sharing my work. This essentially acts as a checklist for all the places I want to share my work on. Here's the template I follow for creating the Google Doc -</p>
<ul>
<li>Link to where the work has been posted.</li>
<li>Brief description of the work.</li>
<li><p>Post table:</p>
<p><img src="https://github.com/sayakpaul/portfolio/blob/master/images/blog_matters.png?raw=true" alt=""></p>
</li>
</ul>
<p>I generally keep the description to a maximum of <em>280 characters</em> so that I can use it on Twitter as well.</p>
<p>Now, turning to the platforms and communities, here are some recommendations (in no particular order):</p>
<ul>
<li>HackerNews (<a href="https://news.ycombinator.com/newest">https://news.ycombinator.com/newest</a>)</li>
<li>Made With ML (<a href="https://madewithml.com/">https://madewithml.com/</a>)</li>
<li>Reddit<ul>
<li><a href="https://www.reddit.com/r/MachineLearning/">r/MachineLearning</a></li>
<li><a href="https://www.reddit.com/r/MachinesLearn/">r/MachinesLearn</a></li>
<li><a href="https://www.reddit.com/r/learnmachinelearning/">r/learnmachinelearning</a></li>
<li><a href="https://www.reddit.com/r/deeplearning/">r/deeplearning</a></li>
</ul>
</li>
<li>Twitter</li>
<li>Facebook <ul>
<li><a href="https://www.facebook.com/groups/DeepNetGroup/">AIDL</a></li>
<li><a href="https://www.facebook.com/groups/MontrealAI/">Montreal AI</a></li>
<li><a href="https://www.facebook.com/groups/DeepLearnng/">Deep Learning</a></li>
</ul>
</li>
<li>Fast.ai Forum (<a href="https://forums.fast.ai/">https://forums.fast.ai/</a>)</li>
<li>LinkedIn</li>
<li>Google Groups (depends on the framework used in the work)<ul>
<li>discuss@tensorflow.org</li>
<li>tflite@tensorflow.org</li>
<li>tfjs@tensorflow.org</li>
<li>tfx@tensorflow.org</li>
</ul>
</li>
</ul>
<p>While sharing my work, I find it to be important to always attach a brief description. Additionally, if your work is related to implementing research work, you should definitely include it on <a href="https://paperswithcode.com/">Papers with Code</a>.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Sharing-to-aid-discussions">Sharing to aid discussions<a class="anchor-link" href="#Sharing-to-aid-discussions"> </a></h2><p>You might be active on online forums like Quora, StackOverflow, and so on. While participating in a discussion in those forums you can make effective use of your work <em>if it is relevant</em>. In these cases, the approach is to not just supply a link to your work, but also to first write about any important pointers relevant to the discussion first, and then supply the link to your work to better aid it. Let's say there's a discussion going on the topic of <em>"What is Weight Initialization in Neural Nets?"</em> Here's how I would approach my comment:</p>
<blockquote><p>A neural net can be viewed as a function with learnable parameters and those parameters are often referred to as weights and biases. Now, while starting the training of neural nets these parameters (typically the weights) are initialized in a number of different ways - sometimes, using constant values like 0’s and 1’s, sometimes with values sampled from some distribution (typically a uniform distribution or normal distribution), sometimes with other sophisticated schemes like Xavier Initialization.
The performance of a neural net depends a lot on how its parameters are initialized when it is starting to train. Moreover, if we initialize it randomly for each run, it’s bound to be non-reproducible (almost) and even not-so-performant too. On the other hand, if we initialize it with constant values, it might take way too long to converge. With that, we also eliminate the beauty of randomness which in turn gives a neural net the power to reach covergence quicker using gradient-based learning. We clearly need a better way to initialize it.
Careful initialization of weights helps us to train them better. To know more, please follow <a href="https://www.wandb.com/articles/the-effects-of-weight-initialization-on-neural-nets">this article of mine</a>.</p>
</blockquote>
<p>Well, that's it for now. I hope it proves to be useful for you. Please provide any suggestions you may have via the comments. I am thankful to <a href="https://www.linkedin.com/in/alessio-gozzoli-530aa2109/">Alessio</a> of <a href="https://www.floydhub.com/">FloydHub</a> for sharing these tips with me.</p>

</div>
</div>
</div>
</div>
 

